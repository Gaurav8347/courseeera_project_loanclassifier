{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this notebook we try to practice all the classification algorithms that we learned in this course.\n\nWe load a dataset using Pandas library, and apply the following algorithms, and find the best one for this specific dataset by accuracy evaluation methods.\n\nLets first load required libraries:"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# About dataset"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "About dataset:This dataset is about past loans. The Loan_Data_set.csv data set includes details of 96 customers whose loan are already paid off or defaulted. It includes following fields:"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Loan_ID: Loan Id\nGender : The gender of applicant\nMarried : The marrital status of applicant\nDependents : The total no of dependents\nEducation :  Graduate or non Graduate\nSelf_Employed : self employed or not\nApplicantIncome : Income of applicant\nCoapplicantIncome : Income of  Coapplicant\nLoanAmount : Loan amount needed\nLoan_Amount_Term : Term of loan amount\nCredit_History : Is there any credit history or not\nProperty_Area : Area of property ,rural or urban or semi urban\nLoan_Status : Loan status yes or not."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Load Data From CSV File"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df = pd.read_csv('loan_data_set.csv')\nprint(df['Loan_Status'].value_counts())\nprint(df.columns)\nprint(df.head())",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Lets define feature sets, X:\n#To use scikit-learn library, we have to convert the Pandas data frame to a Numpy array:\nX = df[['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome',\n        'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']] .values\n\nX[0:5]\ny = df['Loan_Status'].values\ny[0:5]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Data Standardization give data zero mean and unit variance, it is good practice, especially for algorithms such as KNN\n# which is based on distance of cases:\nX = preprocessing.StandardScaler().fit(X).transform(X.astype(str))\nX[0:5]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.model_selection import train_test_split\n#Train Test Split\u00b6\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=4)\nprint('Train set:', X_train.shape,  y_train.shape)\nprint('Test set:', X_test.shape,  y_test.shape)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.neighbors import KNeighborsClassifier\n#Training\n#Lets start the algorithm with k=4 for now:\nk = 3\n#Train Model and Predict\nneigh = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n#Predicting\n#we can use the model to predict the test set:\nyhat = neigh.predict(X_test)\nyhat[0:5]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import metrics\n#Accuracy evaluation\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))\nKs = 10\nmean_acc = np.zeros((Ks - 1))\nstd_acc = np.zeros((Ks - 1))\nConfustionMx = [];\nfor n in range(2, Ks):\n        # Train Model and Predict\n        neigh = KNeighborsClassifier(n_neighbors=n).fit(X_train, y_train)\n        yhat = neigh.predict(X_test)\n        mean_acc[n - 1] = metrics.accuracy_score(y_test, yhat)\n\n        std_acc[n - 1] = np.std(yhat == y_test) / np.sqrt(yhat.shape[0])\n\nmean_acc\n\nplt.plot(range(1, Ks), mean_acc, 'g')\nplt.fill_between(range(1, Ks), mean_acc - 1 * std_acc, mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Nabors (K)')\nplt.tight_layout()\nplt.show()\nprint(\"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax() + 1)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Decision tree "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.tree import DecisionTreeClassifier\nloanTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nloanTree # it shows the default parameters\nloanTree.fit(X_train,y_train)\npredTree = loanTree.predict(X_test)\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predTree))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Support Vector Machine"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import svm\nclf = svm.SVC()\nclf.fit(X_train, y_train)\nyhat = clf.predict(X_test)\nyhat [0:5]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Logistic Regression"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR\nyhat = LR.predict(X_test)\nyhat",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Model Evaluation using Test set\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# 1.Knn"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\n# predicted y\nyhat_knn = neigh.predict(X_test)\n# jaccard\njaccard_knn = jaccard_similarity_score(y_test, yhat_knn)\nprint(\"KNN Jaccard index: \", jaccard_knn)\n# f1_score\nf1_score_knn = f1_score(y_test, yhat_knn, average='weighted')\nprint(\"KNN F1-score: \", f1_score_knn)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# 2.Decision tree"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.tree import DecisionTreeClassifier\n# predicted y\nyhat_dt = loanTree.predict(X_test)\n\n# jaccard\njaccard_dt = jaccard_similarity_score(y_test, yhat_dt)\nprint(\"DT Jaccard index: \", jaccard_dt)\n\n# f1_score\nf1_score_dt = f1_score(y_test, yhat_dt, average='weighted')\nprint(\"DT F1-score: \", f1_score_dt)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# 3.SVM"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import svm\n# predicted y\nyhat_svm = clf.predict(X_test)\n\n# jaccard\njaccard_svm = jaccard_similarity_score(y_test, yhat_svm)\nprint(\"SVM Jaccard index: \", jaccard_svm)\n\n# f1_score\nf1_score_svm = f1_score(y_test, yhat_svm, average='weighted')\nprint(\"SVM F1-score: \", f1_score_svm)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# 4.Logistic regression"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.linear_model import LogisticRegression\n# predicted y\nyhat_lg = LR.predict(X_test)\nyhat_lg_prob = LR.predict_proba(X_test)\n\n# jaccard\njaccard_lg = jaccard_similarity_score(y_test, yhat_lg)\nprint(\"LR Jaccard index: \", jaccard_lg)\n\n# f1_score\nf1_score_lg = f1_score(y_test, yhat_lg, average='weighted')\nprint(\"LR F1-score: \", f1_score_lg)\n\n# logloss\nlogloss_lg = log_loss(y_test, yhat_lg_prob)\nprint(\"LR log loss: \", logloss_lg)",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}